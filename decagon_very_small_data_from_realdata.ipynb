{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.14</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravanv/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "from itertools import combinations\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pixiedust\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import metrics\n",
    "\n",
    "from decagon.deep.optimizer import DecagonOptimizer\n",
    "from decagon.deep.model import DecagonModel\n",
    "from decagon.deep.minibatch import EdgeMinibatchIterator\n",
    "from decagon.utility import rank_metrics, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train on CPU (hide GPU) due to memory constraints\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "# Train on GPU\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "###########################################################\n",
    "#\n",
    "# Functions\n",
    "#\n",
    "###########################################################\n",
    "\n",
    "\n",
    "def get_accuracy_scores(edges_pos, edges_neg, edge_type):\n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    feed_dict.update({placeholders['batch_edge_type_idx']: minibatch.edge_type2idx[edge_type]})\n",
    "    feed_dict.update({placeholders['batch_row_edge_type']: edge_type[0]})\n",
    "    feed_dict.update({placeholders['batch_col_edge_type']: edge_type[1]})\n",
    "    rec = sess.run(opt.predictions, feed_dict=feed_dict)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1. / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    preds = []\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    edge_ind = 0\n",
    "    for u, v in edges_pos[edge_type[:2]][edge_type[2]]:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds.append(score)\n",
    "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 1, 'Problem 1'\n",
    "\n",
    "        actual.append(edge_ind)\n",
    "        predicted.append((score, edge_ind))\n",
    "        edge_ind += 1\n",
    "\n",
    "    preds_neg = []\n",
    "    for u, v in edges_neg[edge_type[:2]][edge_type[2]]:\n",
    "        score = sigmoid(rec[u, v])\n",
    "        preds_neg.append(score)\n",
    "        assert adj_mats_orig[edge_type[:2]][edge_type[2]][u,v] == 0, 'Problem 0'\n",
    "\n",
    "        predicted.append((score, edge_ind))\n",
    "        edge_ind += 1\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    preds_all = np.nan_to_num(preds_all)\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    predicted = list(zip(*sorted(predicted, reverse=True, key=itemgetter(0))))[1]\n",
    "\n",
    "    roc_sc = metrics.roc_auc_score(labels_all, preds_all)\n",
    "    aupr_sc = metrics.average_precision_score(labels_all, preds_all)\n",
    "    apk_sc = rank_metrics.apk(actual, predicted, k=50)\n",
    "\n",
    "    return roc_sc, aupr_sc, apk_sc\n",
    "\n",
    "\n",
    "def construct_placeholders(edge_types):\n",
    "    placeholders = {\n",
    "        'batch': tf.placeholder(tf.int32, name='batch'),\n",
    "        'batch_edge_type_idx': tf.placeholder(tf.int32, shape=(), name='batch_edge_type_idx'),\n",
    "        'batch_row_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_row_edge_type'),\n",
    "        'batch_col_edge_type': tf.placeholder(tf.int32, shape=(), name='batch_col_edge_type'),\n",
    "        'degrees': tf.placeholder(tf.int32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    }\n",
    "    placeholders.update({\n",
    "        'adj_mats_%d,%d,%d' % (i, j, k): tf.sparse_placeholder(tf.float32)\n",
    "        for i, j in edge_types for k in range(edge_types[i,j])})\n",
    "    placeholders.update({\n",
    "        'feat_%d' % i: tf.sparse_placeholder(tf.float32)\n",
    "        for i, _ in edge_types})\n",
    "    return placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "#\n",
    "# Load and preprocess data (This is a dummy toy example!)\n",
    "#\n",
    "###########################################################\n",
    "\n",
    "####\n",
    "# The following code uses artificially generated and very small networks.\n",
    "# Expect less than excellent performance as these random networks do not have any interesting structure.\n",
    "# The purpose of main.py is to show how to use the code!\n",
    "#\n",
    "# All preprocessed datasets used in the drug combination study are at: http://snap.stanford.edu/decagon:\n",
    "# (1) Download datasets from http://snap.stanford.edu/decagon to your local machine.\n",
    "# (2) Replace dummy toy datasets used here with the actual datasets you just downloaded.\n",
    "# (3) Train & test the model.\n",
    "####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bio-decagon-ppi:Protein-protein interaction network\n",
    "#bio-decagon-targets: Drug-target protein associations\n",
    "#bio-decagon-targets-all: Drug-target protein associations culled from several curated databases\n",
    "#bio-decagon-combo:Polypharmacy side effects in the form of (drug A, side effect type, drug B) triples\n",
    "#bio-decagon-effectcategories: Side effect categories\n",
    "#bio-decagon-mono:Side effects of individual drugs in the form of (drug A, side effect type) tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Information about the datasets from the paper:</h1>\n",
    "<h4>The protein-prtoein network:</h4>\n",
    "number of proteins = 19,085, number of physical interactions = 719,402\n",
    "<h4>The drug-prtoein network:</h4>\n",
    "number of proteins = 8,934, number of drugs = 519,022, number of interactions = 8,083,600\n",
    "<h4>The drug-drug network(individuale):</h4>\n",
    "number of  drugs = 1,556, number of side effects = 5,868, number of drug-side effect association = 286,399\n",
    "<h4>The drug-drugn network(combination):</h4>\n",
    "number of  drug combinations = 63,473, number of side effect types = 1,318, number of drug combination-side effect association = 4,651,131\n",
    "<h2>Final Network:</h2>\n",
    "<h3>Number of protiens = 19,085 (paper) ....... Number of protiens = 19,081(ppi data) </h3>\n",
    "<h3>Number of drugs = 645 (paper).......... Number of drugs = 645 (polypharmacy side effect data (combo))</h3>\n",
    "<h3>Number of protien-protien edges= 715,612(paper) ....... Number of protien-protien edges= 715,612 (ppi data)</h3>\n",
    "<h3>Number of drug-drug edges= 4,651,131 (paper) ......... Number of drug-drug edges= 4,649,441 (polypharmacy side effect data (combo)) </h3>\n",
    "<h3>Number of drug-protien edges= 18,596 (paper) ........ Number of drug-protien edges= 18,690 (Drug-target protein (targets))</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#protein_protein_data = pd.read_csv(\"/Users/ravanv/Desktop/Decagon/data/bio-decagon-ppi.csv\", sep=',',header = 0)\n",
    "#drug_target_protein_data = pd.read_csv(\"/Users/ravanv/Desktop/Decagon/data/bio-decagon-targets.csv\", sep=',',header = 0)\n",
    "#drug_target_protein_all_data = pd.read_csv(\"/Users/ravanv/Desktop/Decagon/data/bio-decagon-targets-all.csv\", sep=',',header = 0)\n",
    "#polypharmacy_side_effect_data = pd.read_csv(\"/Users/ravanv/Desktop/Decagon/data/bio-decagon-combo.csv\", sep=',',header = 0)\n",
    "#side_effect_categories_data = pd.read_csv(\"/Users/ravanv/Desktop/Decagon/data/bio-decagon-effectcategories.csv\", sep=',',header = 0)\n",
    "#side_effect_individuale_data = pd.read_csv(\"/Users/ravanv/Desktop/Decagon/data/bio-decagon-mono.csv\", sep=',',header = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Reading 3 small datasets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_protein_data = pd.read_csv(\"/Users/ravanv/Desktop/Decagon/data/gene_gene.csv\", sep=',',header = 0)\n",
    "polypharmacy_side_effect_data = pd.read_csv(\"/Users/ravanv/Desktop/Decagon/data/side_effect.csv\", sep=',',header = 0)\n",
    "drug_target_protein_data = pd.read_csv(\"/Users/ravanv/Desktop/Decagon/data/drug_gene.csv\", sep=',',header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the protein-protien network: (15, 2)\n",
      "size of the drug-target protein associations: (15, 2)\n",
      "size of the polypharmacy side effects: (15, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"size of the protein-protien network:\", protein_protein_data.shape)\n",
    "print(\"size of the drug-target protein associations:\", drug_target_protein_data.shape)\n",
    "#print(\"size of the Drug-target protein associations culled from several curated databases :\",drug_target_protein_all_data.shape)\n",
    "print(\"size of the polypharmacy side effects:\", polypharmacy_side_effect_data.shape)\n",
    "#print(\"size of the Side effect categories:\",side_effect_categories_data.shape)\n",
    "#print(\"size of the Side effects of individual drugs:\",side_effect_individuale_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_test_size = 0.1 #changed from 0.05 to 0.1 in the original code\n",
    "#n_genes = 19081 #from protein-protein network \n",
    "#n_drugs = 645 #from drug-drug combo network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Creating a symmetric adjacency matrix for genes from the protein-protein network </h2>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the gene-gene adjacecny matrix: (23, 23) , number of genes: 23\n"
     ]
    }
   ],
   "source": [
    "df_gene1_gene2 = pd.crosstab(protein_protein_data['Gene 1'], protein_protein_data['Gene 2'])\n",
    "gene_idx = df_gene1_gene2.columns.union(df_gene1_gene2.index)\n",
    "df_gene1_gene2 = df_gene1_gene2.reindex(index = gene_idx, columns = gene_idx,fill_value=0)#upper triangle of the adjacency matrix\n",
    "df_gene2_gene1 = pd.crosstab(protein_protein_data['Gene 2'], protein_protein_data['Gene 1'])\n",
    "df_gene2_gene1 = df_gene2_gene1.reindex(index = gene_idx, columns = gene_idx, fill_value=0)#lower triangle of the adjacency matrix\n",
    "gene_adj = df_gene2_gene1.add(df_gene1_gene2, fill_value=0)#creates a symmetric adjacency matrix of Gene 1 and Gene 2 by adding upper triangle and lower triangle\n",
    "gene_degrees = np.array(gene_adj.sum(axis=0)).squeeze()#get the degrees of genes\n",
    "print(\"size of the gene-gene adjacecny matrix:\",gene_adj.shape, \", number of genes:\",len(gene_degrees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_genes = len(gene_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Making an adjacency matrix for the genes from the adjacency dataframe</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices_genes = list(range(0,len(gene_idx)))\n",
    "dict_genes = dict(zip(gene_idx, indices_genes))#creating a dictionary to map genes to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gene_adj_mat = np.zeros((n_genes,n_genes)) # adj. matrix of size n_genes * n_genes\n",
    "for i in range(0,protein_protein_data.shape[0]):#read from the protein-protein file\n",
    "    gene1 = protein_protein_data.loc[i][0]#read gene1\n",
    "    gene2 = protein_protein_data.loc[i][1]#read gene2\n",
    "    #print(gene1,gene2)\n",
    "    gene1_index = dict_genes.get(gene1)#get the index of gene1 in dictionary\n",
    "    gene2_index = dict_genes.get(gene2)#get the index of gene2 in dictionary\n",
    "    #print(gene1_index,gene2_index)\n",
    "    gene_adj_mat[gene1_index][gene2_index] = 1.0 \n",
    "    gene_adj_mat[gene2_index][gene1_index] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G=nx.from_numpy_matrix(gene_adj_mat)\n",
    "edges = G.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_graph_with_labels(adjacency_matrix):\n",
    "    rows, cols = np.where(adjacency_matrix == 1)\n",
    "    edges = zip(rows.tolist(), cols.tolist())\n",
    "    gr = nx.Graph()\n",
    "    gr.add_edges_from(edges)\n",
    "    nx.draw(gr, node_size=500)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show_graph_with_labels(gene_adj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gene_adj_mat_t = gene_adj_mat.transpose() #transpose matrix of the adj. matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Creating a list of symmetric adjacency matrices for each side effect from the drug-drug network (combo)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_side_effects = []\n",
    "prev_side_effect = []\n",
    "for i in range(0,polypharmacy_side_effect_data.shape[0]):#get the list of side effects\n",
    "    if polypharmacy_side_effect_data.loc[i][2] not in prev_side_effect:\n",
    "        list_side_effects.append(polypharmacy_side_effect_data.loc[i][2])\n",
    "        prev_side_effect.append(polypharmacy_side_effect_data.loc[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of side effects = 15\n"
     ]
    }
   ],
   "source": [
    "print(\"number of side effects =\", len(list_side_effects) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of drugs =  6\n"
     ]
    }
   ],
   "source": [
    "df_drug1_drug2 = pd.crosstab(polypharmacy_side_effect_data['STITCH 1'], polypharmacy_side_effect_data['STITCH 2'])\n",
    "drug_idx = df_drug1_drug2.columns.union(df_drug1_drug2.index)#get the list of all drugs\n",
    "n_drugs = len(drug_idx)\n",
    "print(\"number of drugs = \", len(drug_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices_drugs = list(range(0,len(drug_idx)))\n",
    "dict_drugs = dict(zip(drug_idx, indices_drugs))#create a dictionary to map each drug to its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_drug_adj_list = []\n",
    "for i in range(len(list_side_effects)):\n",
    "    drug_drug_mat = np.zeros((n_drugs,n_drugs))\n",
    "    for row_index in range(polypharmacy_side_effect_data.shape[0]):\n",
    "        if polypharmacy_side_effect_data.loc[row_index][2] == list_side_effects[i]:\n",
    "            drug_1 = polypharmacy_side_effect_data.loc[row_index][0]\n",
    "            drug_2 = polypharmacy_side_effect_data.loc[row_index][1] \n",
    "            drug_1_index = dict_drugs.get(drug_1)\n",
    "            drug_2_index = dict_drugs.get(drug_2)\n",
    "            drug_drug_mat[drug_1_index,drug_2_index] =   drug_drug_mat[drug_2_index,drug_1_index] = 1.\n",
    "    drug_drug_adj_list.append(sp.csr_matrix(drug_drug_mat))\n",
    "drug_degrees_list = [np.array(drug_adj.sum(axis=0)).squeeze() for drug_adj in drug_drug_adj_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Creating adjacency matrices for the drug-protein and protein-drug network.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drug_gene_adj = np.zeros((n_drugs,n_genes))\n",
    "gene_drug_adj = np.zeros((n_genes, n_drugs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0\n",
      "19 0\n",
      "17 0\n",
      "16 0\n",
      "18 0\n",
      "9 0\n",
      "22 0\n",
      "15 3\n",
      "14 3\n",
      "12 3\n",
      "13 3\n",
      "11 3\n",
      "21 3\n",
      "1 2\n",
      "0 2\n"
     ]
    }
   ],
   "source": [
    "#%%pixie_debugger\n",
    "for i in range(0,drug_target_protein_data.shape[0]):\n",
    "    drug = drug_target_protein_data.loc[i][0]\n",
    "    gene = drug_target_protein_data.loc[i][1]\n",
    "    if drug in drug_idx and gene in gene_idx:\n",
    "        gene_index = dict_genes.get(gene)\n",
    "        drug_index = dict_drugs.get(drug)\n",
    "        print(gene_index, drug_index)\n",
    "        drug_gene_adj[drug_index][gene_index] = 1.0\n",
    "        gene_drug_adj[gene_index][drug_index] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_target_protein_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adj_mats_orig = {\n",
    "    (0, 0): [sp.csr_matrix(gene_adj_mat), sp.csr_matrix(gene_adj_mat_t)],\n",
    "    (0, 1): [sp.csr_matrix(gene_drug_adj)],\n",
    "    (1, 0): [sp.csr_matrix(drug_gene_adj)],\n",
    "    (1, 1): drug_drug_adj_list + [x.transpose(copy=True) for x in drug_drug_adj_list],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degrees = {\n",
    "    0: [gene_degrees, gene_degrees],\n",
    "    1: drug_degrees_list + drug_degrees_list,\n",
    "}\n",
    "# featureless (genes)\n",
    "gene_feat = sp.identity(n_genes)\n",
    "gene_nonzero_feat, gene_num_feat = gene_feat.shape\n",
    "gene_feat = preprocessing.sparse_to_tuple(gene_feat.tocoo())\n",
    "# features (drugs)\n",
    "drug_feat = sp.identity(n_drugs)\n",
    "drug_nonzero_feat, drug_num_feat = drug_feat.shape\n",
    "drug_feat = preprocessing.sparse_to_tuple(drug_feat.tocoo())\n",
    "# data representation\n",
    "num_feat = {\n",
    "    0: gene_num_feat,\n",
    "    1: drug_num_feat,\n",
    "}\n",
    "nonzero_feat = {\n",
    "    0: gene_nonzero_feat,\n",
    "    1: drug_nonzero_feat,\n",
    "}\n",
    "feat = {\n",
    "    0: gene_feat,\n",
    "    1: drug_feat,\n",
    "}\n",
    "edge_type2dim = {k: [adj.shape for adj in adjs] for k, adjs in adj_mats_orig.items()}\n",
    "edge_type2decoder = {\n",
    "    (0, 0): 'bilinear',\n",
    "    (0, 1): 'bilinear',\n",
    "    (1, 0): 'bilinear',\n",
    "    (1, 1): 'dedicom',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge types: 34\n",
      "Defining placeholders\n"
     ]
    }
   ],
   "source": [
    "edge_types = {k: len(v) for k, v in adj_mats_orig.items()}\n",
    "num_edge_types = sum(edge_types.values())\n",
    "print(\"Edge types:\", \"%d\" % num_edge_types)\n",
    "\n",
    "###########################################################\n",
    "#\n",
    "# Settings and placeholders\n",
    "#\n",
    "###########################################################\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('neg_sample_size', 1, 'Negative sample size.')\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 50, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_float('weight_decay', 0, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('max_margin', 0.1, 'Max margin parameter in hinge loss')\n",
    "flags.DEFINE_integer('batch_size', 512, 'minibatch size.')\n",
    "flags.DEFINE_boolean('bias', True, 'Bias term.')\n",
    "# Important -- Do not evaluate/print validation performance every iteration as it can take\n",
    "# substantial amount of time\n",
    "PRINT_PROGRESS_EVERY = 150\n",
    "\n",
    "print(\"Defining placeholders\")\n",
    "placeholders = construct_placeholders(edge_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create minibatch iterator\n",
      "Minibatch edge type: (0, 0, 0)\n",
      "Constructing val edges= 0000/0030\n",
      "Train edges= 0000\n",
      "Val edges= 0030\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (0, 0, 1)\n",
      "Constructing val edges= 0000/0030\n",
      "Train edges= 0000\n",
      "Val edges= 0030\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (0, 1, 0)\n",
      "Constructing val edges= 0000/0015\n",
      "Train edges= 0000\n",
      "Val edges= 0015\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 0, 0)\n",
      "Constructing val edges= 0000/0015\n",
      "Train edges= 0000\n",
      "Val edges= 0015\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 0)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 1)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 2)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 3)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 4)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 5)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 6)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 7)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 8)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 9)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 10)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 11)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 12)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 13)\n",
      "Constructing val edges= 0000/0002\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 14)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 15)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 16)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 17)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 18)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 19)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 20)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 21)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 22)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 23)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 24)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 25)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 26)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 27)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 28)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n",
      "Minibatch edge type: (1, 1, 29)\n",
      "Constructing val edges= 0000/0002\n",
      "Train edges= 0000\n",
      "Val edges= 0002\n",
      "Test edges= 0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravanv/Desktop/Decagon/decagon-master/decagon/deep/minibatch.py:67: RuntimeWarning: divide by zero encountered in power\n",
      "  rowdegree_mat_inv = sp.diags(np.nan_to_num(np.power(rowsum, -0.5)).flatten())\n",
      "/Users/ravanv/Desktop/Decagon/decagon-master/decagon/deep/minibatch.py:68: RuntimeWarning: divide by zero encountered in power\n",
      "  coldegree_mat_inv = sp.diags(np.nan_to_num(np.power(colsum, -0.5)).flatten())\n"
     ]
    }
   ],
   "source": [
    "print(\"Create minibatch iterator\")\n",
    "minibatch = EdgeMinibatchIterator(\n",
    "    adj_mats=adj_mats_orig,\n",
    "    feat=feat,\n",
    "    edge_types=edge_types,\n",
    "    batch_size=FLAGS.batch_size,\n",
    "    val_test_size=val_test_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model\n"
     ]
    }
   ],
   "source": [
    "print(\"Create model\")\n",
    "model = DecagonModel(\n",
    "    placeholders=placeholders,\n",
    "    num_feat=num_feat,\n",
    "    nonzero_feat=nonzero_feat,\n",
    "    edge_types=edge_types,\n",
    "    decoders=edge_type2decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create optimizer\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravanv/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "print(\"Create optimizer\")\n",
    "with tf.name_scope('optimizer'):\n",
    "    opt = DecagonOptimizer(\n",
    "        embeddings=model.embeddings,\n",
    "        latent_inters=model.latent_inters,\n",
    "        latent_varies=model.latent_varies,\n",
    "        degrees=degrees,\n",
    "        edge_types=edge_types,\n",
    "        edge_type2dim=edge_type2dim,\n",
    "        placeholders=placeholders,\n",
    "        batch_size=FLAGS.batch_size,\n",
    "        margin=FLAGS.max_margin\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize session\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize session\")\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "feed_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "iteration: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-838be5f46183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iteration:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Construct feed dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_minibatch_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaceholders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplaceholders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         feed_dict = minibatch.update_feed_dict(\n\u001b[1;32m     17\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Decagon/decagon-master/decagon/deep/minibatch.py\u001b[0m in \u001b[0;36mnext_minibatch_feed_dict\u001b[0;34m(self, placeholders)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_edge_type_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "#\n",
    "# Train model\n",
    "#\n",
    "###########################################################\n",
    "\n",
    "print(\"Train model\")\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    minibatch.shuffle()\n",
    "    itr = 0\n",
    "    while not minibatch.end():\n",
    "        print(\"iteration:\", itr)\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = minibatch.next_minibatch_feed_dict(placeholders=placeholders)\n",
    "        feed_dict = minibatch.update_feed_dict(\n",
    "            feed_dict=feed_dict,\n",
    "            dropout=FLAGS.dropout,\n",
    "            placeholders=placeholders)\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        # Training step: run single weight update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.batch_edge_type_idx], feed_dict=feed_dict)\n",
    "        train_cost = outs[1]\n",
    "        batch_edge_type = outs[2]\n",
    "\n",
    "        if itr % PRINT_PROGRESS_EVERY == 0:\n",
    "            val_auc, val_auprc, val_apk = get_accuracy_scores(\n",
    "                minibatch.val_edges, minibatch.val_edges_false,\n",
    "                minibatch.idx2edge_type[minibatch.current_edge_type_idx])\n",
    "\n",
    "            print(\"Epoch:\", \"%04d\" % (epoch + 1), \"Iter:\", \"%04d\" % (itr + 1), \"Edge:\", \"%04d\" % batch_edge_type,\n",
    "                  \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
    "                  \"val_roc=\", \"{:.5f}\".format(val_auc), \"val_auprc=\", \"{:.5f}\".format(val_auprc),\n",
    "                  \"val_apk=\", \"{:.5f}\".format(val_apk), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "        itr += 1\n",
    "\n",
    "print(\"Optimization finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for et in range(num_edge_types):\n",
    "    roc_score, auprc_score, apk_score = get_accuracy_scores(\n",
    "        minibatch.test_edges, minibatch.test_edges_false, minibatch.idx2edge_type[et])\n",
    "    print(\"Edge type=\", \"[%02d, %02d, %02d]\" % minibatch.idx2edge_type[et])\n",
    "    print(\"Edge type:\", \"%04d\" % et, \"Test AUROC score\", \"{:.5f}\".format(roc_score))\n",
    "    print(\"Edge type:\", \"%04d\" % et, \"Test AUPRC score\", \"{:.5f}\".format(auprc_score))\n",
    "    print(\"Edge type:\", \"%04d\" % et, \"Test AP@k score\", \"{:.5f}\".format(apk_score))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
